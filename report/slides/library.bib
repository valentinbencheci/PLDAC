@inproceedings{adavanneStackedConvolutionalRecurrent2017,
  title = {Stacked Convolutional and Recurrent Neural Networks for Bird Audio Detection},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Adavanne, Sharath and Drossos, Konstantinos and {\c C}akir, Emre and Virtanen, Tuomas},
  year = {2017},
  month = aug,
  pages = {1729--1733},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO.2017.8081505},
  abstract = {This paper studies the detection of bird calls in audio segments using stacked convolutional and recurrent neural networks. Data augmentation by blocks mixing and domain adaptation using a novel method of test mixing are proposed and evaluated in regard to making the method robust to unseen data. The contributions of two kinds of acoustic features (dominant frequency and log mel-band energy) and their combinations are studied in the context of bird audio detection. Our best achieved AUC measure on five cross-validations of the development data is 95.5\% and 88.1\% on the unseen evaluation data.},
  keywords = {Birds,Feature extraction,Harmonic analysis,Recurrent neural networks,Training},
  file = {files/116/Adavanne et al. - 2017 - Stacked convolutional and recurrent neural network.pdf}
}

@article{brookerAutomatedDetectionClassification2020,
  title = {Automated Detection and Classification of Birdsong: {{An}} Ensemble Approach},
  shorttitle = {Automated Detection and Classification of Birdsong},
  author = {Brooker, Stuart A. and Stephens, Philip A. and Whittingham, Mark J. and Willis, Stephen G.},
  year = {2020},
  month = oct,
  journal = {Ecological Indicators},
  volume = {117},
  pages = {106609},
  issn = {1470-160X},
  doi = {10.1016/j.ecolind.2020.106609},
  urldate = {2023-03-21},
  abstract = {The avian dawn chorus presents a challenging opportunity to test autonomous recording units (ARUs) and associated recogniser software in the types of complex acoustic environments frequently encountered in the natural world. To date, extracting information from acoustic surveys using readily-available signal recognition tools (`recognisers') for use in biodiversity surveys has met with limited success. Combining signal detection methods used by different recognisers could improve performance, but this approach remains untested. Here, we evaluate the ability of four commonly used and commercially- or freely-available individual recognisers to detect species, focusing on five woodland birds with widely-differing song-types. We combined the likelihood scores (of a vocalisation originating from a target species) assigned to detections made by the four recognisers to devise an ensemble approach to detecting and classifying birdsong. We then assessed the relative performance of individual recognisers and that of the ensemble models. The ensemble models out-performed the individual recognisers across all five song-types, whilst also minimising false positive error rates for all species tested. Moreover, during acoustically complex dawn choruses, with many species singing in parallel, our ensemble approach resulted in detection of 74\% of singing events, on average, across the five song-types, compared to 59\% when averaged across the recognisers in isolation; a marked improvement. We suggest that this ensemble approach, used with suitably trained individual recognisers, has the potential to finally open up the use of ARUs as a means of automatically detecting the occurrence of target species and identifying patterns in singing activity over time in challenging acoustic environments.},
  langid = {english},
  keywords = {Automated detection,Bioacoustics,Birdsong,Dawn chorus,Ensemble forecasting,Survey method},
  file = {files/45/Brooker et al. - 2020 - Automated detection and classification of birdsong.pdf;files/46/S1470160X2030546X.html}
}

@inproceedings{cakirConvolutionalRecurrentNeural2017a,
  title = {Convolutional Recurrent Neural Networks for Bird Audio Detection},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Cakir, Emre and Adavanne, Sharath and Parascandolo, Giambattista and Drossos, Konstantinos and Virtanen, Tuomas},
  year = {2017},
  month = aug,
  pages = {1744--1748},
  publisher = {{IEEE}},
  address = {{Kos, Greece}},
  doi = {10.23919/EUSIPCO.2017.8081508},
  urldate = {2023-03-21},
  abstract = {Bird sounds possess distinctive spectral structure which may exhibit small shifts in spectrum depending on the bird species and environmental conditions. In this paper, we propose using convolutional recurrent neural networks on the task of automated bird audio detection in real-life environments. In the proposed method, convolutional layers extract high dimensional, local frequency shift invariant features, while recurrent layers capture longer term dependencies between the features extracted from short time frames. This method achieves 88.5\% Area Under ROC Curve (AUC) score on the unseen evaluation data and obtains the second place in the Bird Audio Detection challenge.},
  isbn = {978-0-9928626-7-1},
  langid = {english},
  file = {files/143/Cakir et al. - 2017 - Convolutional recurrent neural networks for bird a.pdf}
}

@misc{fazekaMultimodalDeepNeural2018,
  title = {A {{Multi-modal Deep Neural Network}} Approach to {{Bird-song}} Identification},
  author = {Fazeka, Botond and Schindler, Alexander and Lidy, Thomas and Rauber, Andreas},
  year = {2018},
  month = nov,
  number = {arXiv:1811.04448},
  eprint = {arXiv:1811.04448},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.04448},
  urldate = {2023-03-21},
  abstract = {We present a multi-modal Deep Neural Network (DNN) approach for bird song identification. The presented approach takes both audio samples and metadata as input. The audio is fed into a Convolutional Neural Network (CNN) using four convolutional layers. The additionally provided metadata is processed using fully connected layers. The flattened convolutional layers and the fully connected layer of the metadata are joined and fed into a fully connected layer. The resulting architecture achieved 2., 3. and 4. rank in the BirdCLEF2017 task in various training configurations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: LifeCLEF 2017 working notes, Dublin, Ireland},
  file = {files/56/Fazeka et al. - 2018 - A Multi-modal Deep Neural Network approach to Bird.pdf;files/59/1811.html}
}

@inproceedings{gemmekeAudioSetOntology2017,
  title = {Audio {{Set}}: {{An}} Ontology and Human-Labeled Dataset for Audio Events},
  shorttitle = {Audio {{Set}}},
  booktitle = {Proc. {{IEEE ICASSP}} 2017},
  author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  year = {2017},
  address = {{New Orleans, LA}},
  file = {files/71/Gemmeke et al. - 2017 - Audio Set An ontology and human-labeled dataset f.pdf}
}

@article{goeauOverviewBirdCLEF20182018,
  title = {Overview of {{BirdCLEF}} 2018: Monospecies vs. Soundscape Bird Identification},
  author = {Goeau, Herve and Kahl, Stefan and Glotin, Herve and Planque, Robert and Joly, Alexis},
  year = {2018},
  abstract = {The BirdCLEF challenge offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings of their sounds. One of its strengths is that it uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that BirdCLEF is close to the conditions of realworld application, in particular with regard to the number of species in the training set (1500). Two main scenarios are evaluated: (i) the identification of a particular bird species in a recording, and (ii), the recognition of all species vocalising in a long sequence (up to one hour) of raw soundscapes that can contain tens of birds singing more or less simultaneously. This paper reports an overview of the systems developed by the six participating research groups, the methodology of the evaluation of their performance, and an analysis and discussion of the results obtained.},
  langid = {english},
  file = {files/62/Goeau et al. - Overview of BirdCLEF 2018 monospecies vs. soundsc.pdf}
}

@inproceedings{grillTwoConvolutionalNeural2017,
  title = {Two Convolutional Neural Networks for Bird Detection in Audio Signals},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Grill, Thomas and Schluter, Jan},
  year = {2017},
  month = aug,
  pages = {1764--1768},
  publisher = {{IEEE}},
  address = {{Kos, Greece}},
  doi = {10.23919/EUSIPCO.2017.8081512},
  urldate = {2023-03-21},
  abstract = {We present and compare two approaches to detect the presence of bird calls in audio recordings using convolutional neural networks on mel spectrograms. In a signal processing challenge using environmental recordings from three very different sources, only two of them available for supervised training, we obtained an Area Under Curve (AUC) measure of 89\% on the hidden test set, higher than any other contestant. By comparing multiple variations of our systems, we find that despite very different architectures, both approaches can be tuned to perform equally well. Further improvements will likely require a radically different approach to dealing with the discrepancy between data sources.},
  isbn = {978-0-9928626-7-1},
  langid = {english},
  keywords = {Birds,Computer architecture,Convolution,Spectrogram,Training,Training data},
  file = {files/100/Grill et Schluter - 2017 - Two convolutional neural networks for bird detecti.pdf}
}

@misc{hersheyCNNArchitecturesLargeScale2017,
  title = {{{CNN Architectures}} for {{Large-Scale Audio Classification}}},
  author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
  year = {2017},
  month = jan,
  number = {arXiv:1609.09430},
  eprint = {arXiv:1609.09430},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.09430},
  urldate = {2023-03-21},
  abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Statistics - Machine Learning},
  note = {Comment: Accepted for publication at ICASSP 2017 Changes: Added definitions of mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on changes of latest Audio Set revision. Changed wording to fit 4 page limit with new additions},
  file = {files/68/Hershey et al. - 2017 - CNN Architectures for Large-Scale Audio Classifica.pdf;files/69/1609.html}
}

@article{jancovicAutomaticDetectionRecognition2011,
  title = {Automatic {{Detection}} and {{Recognition}} of {{Tonal Bird Sounds}} in {{Noisy Environments}}},
  author = {Jan{\v c}ovi{\v c}, Peter and K{\"o}k{\"u}er, M{\"u}nevver},
  year = {2011},
  month = dec,
  journal = {EURASIP J. Adv. Signal Process.},
  volume = {2011},
  number = {1},
  pages = {1--10},
  publisher = {{SpringerOpen}},
  issn = {1687-6180},
  doi = {10.1155/2011/982936},
  urldate = {2023-03-21},
  abstract = {This paper presents a study of automatic detection and recognition of tonal bird sounds in noisy environments. The detection of spectro-temporal regions containing bird tonal vocalisations is based on exploiting the spectral shape to identify sinusoidal components in the short-time spectrum. The detection method provides tonal-based feature representation that is employed for automatic bird recognition. The recognition system uses Gaussian mixture models to model 165 different bird syllables, produced by 95 bird species. Standard models, as well as models compensating for the effect of the noise, are employed. Experiments are performed on bird sound recordings corrupted by White noise and real-world environmental noise. The proposed detection method shows high detection accuracy of bird tonal components. The employed tonal-based features show significant recognition accuracy improvements over the Mel-frequency cepstral coefficients, in both standard and noise-compensated models, and strong robustness to mismatch between the training and testing conditions.},
  copyright = {2011 Peter Jan\v{c}ovi\v{c} and M\"unevver K\"ok\"uer.},
  langid = {english},
  file = {files/138/Jančovič et Köküer - 2011 - Automatic Detection and Recognition of Tonal Bird .pdf}
}

@article{kahlOverviewBirdCLEF20192019,
  title = {Overview of {{BirdCLEF}} 2019: {{Large-Scale Bird Recognition}} in {{Soundscapes}}},
  author = {Kahl, Stefan and Stoter, Fabian-Robert and Goeau, Herve and Glotin, Herve and Vellinga, Willem-Pier and Joly, Alexis},
  year = {2019},
  abstract = {The BirdCLEF challenge\textemdash as part of the 2019 LifeCLEF Lab [7]\textemdash offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings. The challenge uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that BirdCLEF is close to the conditions of real-world application, in particular with regard to the number of species in the training set (659). In 2019, the challenge was focused on the difficult task of recognizing all birds vocalizing in omni-directional soundscape recordings. Therefore, the dataset of the previous year was extended with more than 350 hours of manually annotated soundscapes that were recorded using 30 field recorders in Ithaca (NY, USA). This paper describes the methodology of the conducted evaluation as well as the synthesis of the main results and lessons learned.},
  langid = {english},
  file = {files/51/Kahl et al. - Overview of BirdCLEF 2019 Large-Scale Bird Recogn.pdf}
}

@article{kohBirdSoundClassification2019,
  title = {Bird {{Sound Classification}} Using {{Convolutional Neural Networks}}},
  author = {Koh, Chih-Yuan and Chang, Jaw-Yuan and Tai, Chiang-Lin and Huang, Da-Yo and Hsieh, Han-Hsing and Liu, Yi-Wen},
  year = {2019},
  abstract = {Accurate prediction of bird species from audio recordings is beneficial to bird conservation. Thanks to the rapid advance in deep learning, the accuracy of bird species identification from audio recordings has greatly improved in recent years. This year, the BirdCLEF2019[4] task invited participants to design a system that could recognize 659 bird species from 50,000 audio recordings. The challenges in this competition included memory management, the number of bird species for the machine to recognize, and the mismatch in signal-to-noise ratio between the training and the testing sets. To participate in this competition, we adopted two recently popular convolutional neural network architectures \textemdash{} the ResNet[1] and the inception model[13]. The inception model achieved 0.16 classification mean average precision (c-mAP) and ranked the second place among five teams that successfully submitted their predictions.},
  langid = {english},
  file = {files/49/Koh et al. - Bird Sound Classiﬁcation using Convolutional Neura.pdf}
}

@inproceedings{kongJointDetectionClassification2017,
  title = {Joint Detection and Classification Convolutional Neural Network on Weakly Labelled Bird Audio Detection},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Kong, Qiuqiang and Xu, Yong and Plumbley, Mark D.},
  year = {2017},
  month = aug,
  pages = {1749--1753},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO.2017.8081509},
  abstract = {Bird audio detection (BAD) aims to detect whether there is a bird call in an audio recording or not. One difficulty of this task is that the bird sound datasets are weakly labelled, that is only the presence or absence of a bird in a recording is known, without knowing when the birds call. We propose to apply joint detection and classification (JDC) model on the weakly labelled data (WLD) to detect and classify an audio clip at the same time. First, we apply VGG like convolutional neural network (CNN) on mel spectrogram as baseline. Then we propose a JDC-CNN model with VGG as a classifier and CNN as a detector. We report the denoising method including optimally-modified log-spectral amplitude (OM-LSA), median filter and spectral spectrogram will worse the classification accuracy on the contrary to previous work. JDC-CNN can predict the time stamps of the events from weakly labelled data, so is able to do sound event detection from WLD. We obtained area under curve (AUC) of 95.70\% on the development data and 81.36\% on the unseen evaluation data, which is nearly comparable to the baseline CNN model.},
  file = {files/111/Kong et al. - 2017 - Joint detection and classification convolutional n.pdf;files/112/8081509.html}
}

@article{lasseckAudiobasedBirdSpecies2018,
  title = {Audio-Based {{Bird Species Identification}} with {{Deep Convolutional Neural Networks}}},
  author = {Lasseck, Mario},
  year = {2018},
  abstract = {This paper presents deep learning techniques for audio-based bird identification at very large scale. Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 1500 species. Various data augmentation techniques are applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the BirdCLEF 2018 campaign and provides the best system in all subtasks. It surpasses previous state-of-the-art by 15.8 \% identifying foreground species and 20.2 \% considering also background species achieving a mean reciprocal rank (MRR) of 82.7 \% and 74.0 \% on the official BirdCLEF Subtask1 test set.},
  langid = {english},
  file = {files/57/Lasseck - Audio-based Bird Species Identification with Deep .pdf}
}

@inproceedings{lasseckAcousticBirdDetection2018,
  title = {Acoustic Bird Detection with Deep Convolutional Neural Networks},
  booktitle = {Workshop on {{Detection}} and {{Classification}} of {{Acoustic Scenes}} and {{Events}}},
  author = {Lasseck, Mario},
  year = {2018},
  urldate = {2023-05-23},
  abstract = {This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated in the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It provides the best system for the task and surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 \% on the public challenge leaderboard.},
  file = {C\:\\Users\\Valinquish\\Zotero\\storage\\9FHX9LUL\\Lasseck - 2018 - Acoustic bird detection with deep convolutional ne.pdf}
}

@misc{liDomesticActivityClustering2022,
  title = {Domestic {{Activity Clustering}} from {{Audio}} via {{Depthwise Separable Convolutional Autoencoder Network}}},
  author = {Li, Yanxiong and Cao, Wenchang and Drossos, Konstantinos and Virtanen, Tuomas},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02406},
  eprint = {arXiv:2208.02406},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.02406},
  urldate = {2023-03-21},
  abstract = {Automatic estimation of domestic activities from audio can be used to solve many problems, such as reducing the labor cost for nursing the elderly people. This study focuses on solving the problem of domestic activity clustering from audio. The target of domestic activity clustering is to cluster audio clips which belong to the same category of domestic activity into one cluster in an unsupervised way. In this paper, we propose a method of domestic activity clustering using a depthwise separable convolutional autoencoder network. In the proposed method, initial embeddings are learned by the depthwise separable convolutional autoencoder, and a clustering-oriented loss is designed to jointly optimize embedding refinement and cluster assignment. Different methods are evaluated on a public dataset (a derivative of the SINS dataset) used in the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) in 2018. Our method obtains the normalized mutual information (NMI) score of 54.46\%, and the clustering accuracy (CA) score of 63.64\%, and outperforms state-of-the-art methods in terms of NMI and CA. In addition, both computational complexity and memory requirement of our method is lower than that of previous deep-model-based methods. Codes: https://github.com/vinceasvp/domestic-activity-clustering-from-audio},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: 6 pages, 5 figures, 4 tables. Accepted by IEEE MMSP 2022},
  file = {files/129/Li et al. - 2022 - Domestic Activity Clustering from Audio via Depthw.pdf;files/130/2208.html}
}

@inproceedings{mcfeeLibrosaAudioMusic2015,
  title = {Librosa: {{Audio}} and {{Music Signal Analysis}} in {{Python}}},
  shorttitle = {Librosa},
  booktitle = {Python in {{Science Conference}}},
  author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  year = {2015},
  pages = {18--24},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-7b98e3ed-003},
  urldate = {2023-03-21},
  abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
  langid = {english},
  file = {files/135/McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf}
}

@misc{naithaniSubjectiveEvaluationDeep2022,
  title = {Subjective {{Evaluation}} of {{Deep Neural Network Based Speech Enhancement Systems}} in {{Real-World Conditions}}},
  author = {Naithani, Gaurav and Pietil{\"a}, Kirsi and Niemist{\"o}, Riitta and Paajanen, Erkki and Takala, Tero and Virtanen, Tuomas},
  year = {2022},
  month = aug,
  number = {arXiv:2208.05057},
  eprint = {arXiv:2208.05057},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.05057},
  urldate = {2023-03-21},
  abstract = {Subjective evaluation results for two low-latency deep neural networks (DNN) are compared to a matured version of a traditional Wiener-filter based noise suppressor. The target use-case is real-world single-channel speech enhancement applications, e.g., communications. Real-world recordings consisting of additive stationary and non-stationary noise types are included. The evaluation is divided into four outcomes: speech quality, noise transparency, speech intelligibility or listening effort, and noise level w.r.t. speech. It is shown that DNNs improve noise suppression in all conditions in comparison to the traditional Wiener-filter baseline without major degradation in speech quality and noise transparency while maintaining speech intelligibility better than the baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: Accepted for publication in IEEE MMSP 2022},
  file = {files/133/Naithani et al. - 2022 - Subjective Evaluation of Deep Neural Network Based.pdf;files/134/2208.html}
}

@inproceedings{narasimhanSimultaneousSegmentationClassification2017,
  title = {Simultaneous Segmentation and Classification of Bird Song Using {{CNN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Narasimhan, Revathy and Fern, Xiaoli Z. and Raich, Raviv},
  year = {2017},
  month = mar,
  pages = {146--150},
  publisher = {{IEEE}},
  address = {{New Orleans, LA}},
  doi = {10.1109/ICASSP.2017.7952135},
  urldate = {2023-03-21},
  abstract = {In bioacoustics, automatic animal voice detection and recognition from audio recordings is an emerging topic for animal preservation. Our research focuses on bird bioacoustics, where the goal is to segment bird syllables from the recording and predict the bird species for the syllables. Traditional methods for this task addresses the segmentation and species prediction separately, leading to propagated errors. This work presents a new approach that performs simultaneous segmentation and classification of bird species using a Convolutional Neural Network (CNN) with encoder-decoder architecture. Experimental results on bird recordings show significant improvement compared to recent state-of-the-art methods for both segmentation and species classification.},
  isbn = {978-1-5090-4117-6},
  langid = {english},
  file = {files/101/Narasimhan et al. - 2017 - Simultaneous segmentation and classification of bi.pdf}
}

@inproceedings{nicholsonComparisonMachineLearning2016,
  title = {Comparison of Machine Learning Methods Applied to Birdsong Element Classification},
  booktitle = {Python in {{Science Conference}}},
  author = {Nicholson, David},
  year = {2016},
  pages = {57--61},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-629e541a-008},
  urldate = {2023-03-21},
  langid = {english},
  file = {files/74/Nicholson - 2016 - Comparison of machine learning methods applied to .pdf}
}

@inproceedings{pellegriniDenselyConnectedCNNs2017,
  title = {Densely Connected {{CNNs}} for Bird Audio Detection},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Pellegrini, Thomas},
  year = {2017},
  month = aug,
  pages = {1734--1738},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO.2017.8081506},
  abstract = {Detecting bird sounds in audio recordings automatically, if accurate enough, is expected to be of great help to the research community working in bio- and ecoacoustics, interested in monitoring biodiversity based on audio field recordings. To estimate how accurate the state-of-the-art machine learning approaches are, the Bird Audio Detection challenge involving large audio datasets was recently organized. In this paper, experiments using several types of convolutional neural networks (i.e. standard CNNs, residual nets and densely connected nets) are reported in the framework of this challenge. DenseNets were the preferred solution since they were the best performing and most compact models, leading to a 88.22\% area under the receiver operator curve score on the test set of the challenge (ranked 3rd/30)1. Performance gains were obtained thank to data augmentation through time and frequency shifting, model parameter averaging during training and ensemble methods using the geometric mean. On the contrary, the attempts to enlarge the training dataset with samples of the test set with automatic predictions used as pseudo-groundtruth labels consistently degraded performance.},
  keywords = {Birds,Convolution,Feature extraction,Standards,Time-frequency analysis,Training},
  file = {files/115/Pellegrini - 2017 - Densely connected CNNs for bird audio detection.pdf;files/121/8081506.html}
}

@article{priyadarshaniAutomatedBirdsongRecognition2018,
  title = {Automated Birdsong Recognition in Complex Acoustic Environments: A Review},
  shorttitle = {Automated Birdsong Recognition in Complex Acoustic Environments},
  author = {Priyadarshani, Nirosha and Marsland, Stephen and Castro, Isabel},
  year = {2018},
  journal = {Journal of Avian Biology},
  volume = {49},
  number = {5},
  pages = {jav-01447},
  issn = {1600-048X},
  doi = {10.1111/jav.01447},
  urldate = {2023-03-21},
  abstract = {Conservationists are increasingly using autonomous acoustic recorders to determine the presence/absence and the abundance of bird species. Unlike humans, these recorders can be left in the field for extensive periods of time in any habitat. Although data acquisition is automated, manual processing of recordings is labour intensive, tedious, and prone to bias due to observer variations. Hence automated birdsong recognition is an efficient alternative. However, only few ecologists and conservationists utilise the existing birdsong recognisers to process unattended field recordings because the software calibration time is exceptionally high and requires considerable knowledge in signal processing and underlying systems, making the tools less user-friendly. Even allowing for these difficulties, getting accurate results is exceedingly hard. In this review we examine the state-of-the-art, summarising and discussing the methods currently available for each of the essential parts of a birdsong recogniser, and also available software. The key reasons behind poor automated recognition are that field recordings are very noisy, calls from birds that are a long way from the recorder can be faint or corrupted, and there are overlapping calls from many different birds. In addition, there can be large numbers of different species calling in one recording, and therefore the method has to scale to large numbers of species, or at least avoid misclassifying another species as one of particular interest. We found that these areas of importance, particularly the question of noise reduction, are amongst the least researched. In cases where accurate recognition of individual species is essential, such as in conservation work, we suggest that specialised (species-specific) methods of passive acoustic monitoring are required. We also believe that it is important that comparable measures, and datasets, are used to enable methods to be compared.},
  langid = {english},
  keywords = {birdsong recognition,birdsong recording,machine learning,noise,passive acoustic monitoring},
  file = {files/61/Priyadarshani et al. - 2018 - Automated birdsong recognition in complex acoustic.pdf;files/64/jav.html}
}

@article{priyadarshaniWaveletFiltersAutomated2020,
  title = {Wavelet Filters for Automated Recognition of Birdsong in Long-Time Field Recordings},
  author = {Priyadarshani, Nirosha and Marsland, Stephen and Juodakis, Julius and Castro, Isabel and Listanti, Virginia},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {3},
  pages = {403--417},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13357},
  urldate = {2023-03-21},
  abstract = {Ecoacoustics has the potential to provide a large amount of information about the abundance of many animal species at a relatively low cost. Acoustic recording units are widely used in field data collection, but the facilities to reliably process the data recorded \textendash{} recognizing calls that are relatively infrequent, and often significantly degraded by noise and distance to the microphone \textendash{} are not well-developed yet. We propose a call detection method for continuous field recordings that can be trained quickly and easily on new species, and degrades gracefully with increased noise or distance from the microphone. The method is based on the reconstruction of the sound from a subset of the wavelet nodes (elements in the wavelet packet decomposition tree). It is intended as a preprocessing filter, therefore we aim to minimize false negatives: false positives can be removed in subsequent processing, but missed calls will not be looked at again. We compare our method to standard call detection methods, and also to machine learning methods (using as input features either wavelet energies or Mel-Frequency Cepstral Coefficients) on real-world noisy field recordings of six bird species. The results show that our method has higher recall (proportion detected) than the alternative methods: 87\% with 85\% specificity on {$>$}53 hr of test data, resulting in an 80\% reduction in the amount of data that needed further verification. It detected {$>$}60\% of calls that were extremely faint (far away), even with high background noise. This preprocessing method is available in our AviaNZ bioacoustic analysis program and enables the user to significantly reduce the amount of subsequent processing required (whether manual or automatic) to analyse continuous field recordings collected by spatially and temporally large-scale monitoring of animal species. It can be trained to recognize new species without difficulty, and if several species are sought simultaneously, filters can be run in parallel.},
  langid = {english},
  keywords = {acoustic surveys,automated birdsong recognition,ecoacoustics,false negatives,field recordings,machine learning,wavelet filters,wavelets},
  file = {files/42/Priyadarshani et al. - 2020 - Wavelet filters for automated recognition of birds.pdf;files/43/2041-210X.html}
}

@inproceedings{salamonFusingShallowDeep2017,
  title = {Fusing Shallow and Deep Learning for Bioacoustic Bird Species Classification},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Salamon, Justin and Bello, Juan Pablo and Farnsworth, Andrew and Kelling, Steve},
  year = {2017},
  month = mar,
  pages = {141--145},
  publisher = {{IEEE}},
  address = {{New Orleans, LA}},
  doi = {10.1109/ICASSP.2017.7952134},
  urldate = {2023-03-21},
  abstract = {Automated c1assification of organisms to species based on their vocalizations would contribute tremendously to abilities to monitor biodiversity, with a wide range of applications in the field of ecology. In particular, automated c1assification of migrating birds' flight calls could yield new biological insights and conservation applications for birds that vocalize during migration. In this paper we explore stateof-the-art c1assification techniques for large-vocabulary bird species c1assification from flight calls. In particular, we contrast a "shallow learning" approach based on unsupervised dictionary learning with a deep convolutional neural network combined with data augmentation. We show that the two models perform comparably on a dataset of 5428 flight calls spanning 43 different species, with both significantly outperforming an MFCC baseline. Finally, we show that by combining the models using a simple late-fusion approach we can further improve the results, obtaining a state-of-the-art c1assification accuracy of 0.96.},
  isbn = {978-1-5090-4117-6},
  langid = {english},
  file = {files/102/Salamon et al. - 2017 - Fusing shallow and deep learning for bioacoustic b.pdf}
}

@article{sprengelAudioBasedBird2016,
  title = {Audio {{Based Bird Species Identification}} Using {{Deep Learning Techniques}}},
  author = {Sprengel, Elias and Jaggi, Martin and Kilcher, Yannic and Hofmann, Thomas},
  year = {2016},
  abstract = {In this paper we present a new audio classification method for bird species identification. Whereas most approaches apply nearest neighbour matching [6] or decision trees [8] using extracted templates for each bird species, ours draws upon techniques from speech recognition and recent advances in the domain of deep learning. With novel preprocessing and data augmentation methods, we train a convolutional neural network on the biggest publicly available dataset [5]. Our network architecture achieves a mean average precision score of 0.686 when predicting the main species of each sound file and scores 0.555 when background species are used as additional prediction targets. As this performance surpasses current state of the art results, our approach won this years international BirdCLEF 2016 Recognition Challenge [3,4,1].},
  langid = {english},
  file = {files/76/Sprengel et al. - Audio Based Bird Species Identiﬁcation using Deep .pdf}
}

@article{stowellAutomaticAcousticDetection2019,
  title = {Automatic Acoustic Detection of Birds through Deep Learning: {{The}} First {{Bird Audio Detection}} Challenge},
  shorttitle = {Automatic Acoustic Detection of Birds through Deep Learning},
  author = {Stowell, Dan and Wood, Michael D. and Pamu{\l}a, Hanna and Stylianou, Yannis and Glotin, Herv{\'e}},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {3},
  pages = {368--380},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13103},
  urldate = {2023-03-21},
  abstract = {Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88\% area under the receiver operating characteristic (ROC) curve (AUC), much higher performance than previous general-purpose methods. With modern machine learning, including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data, with no manual recalibration, and no pretraining of the detector for the target species or the acoustic conditions in the target environment.},
  langid = {english},
  keywords = {bird,deep learning,machine learning,passive acoustic monitoring,sound},
  file = {files/48/Stowell et al. - 2019 - Automatic acoustic detection of birds through deep.pdf;files/52/2041-210X.html}
}

@misc{stowellBirdDetectionAudio2016,
  title = {Bird Detection in Audio: A Survey and a Challenge},
  shorttitle = {Bird Detection in Audio},
  author = {Stowell, Dan and Wood, Mike and Stylianou, Yannis and Glotin, Herv{\'e}},
  year = {2016},
  month = aug,
  number = {arXiv:1608.03417},
  eprint = {arXiv:1608.03417},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.03417},
  urldate = {2023-03-21},
  abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Sound},
  note = {Comment: Slightly extended preprint of paper accepted for MLSP 2016
\par
Comment: Slightly extended preprint of paper accepted for MLSP 2016},
  file = {files/81/Stowell et al. - 2016 - Bird detection in audio a survey and a challenge.pdf;files/82/1608.html}
}

@article{stowellDetectionClassificationAcoustic2015,
  title = {Detection and {{Classification}} of {{Acoustic Scenes}} and {{Events}}},
  author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
  year = {2015},
  month = oct,
  journal = {IEEE Transactions on Multimedia},
  volume = {17},
  number = {10},
  pages = {1733--1746},
  issn = {1941-0077},
  doi = {10.1109/TMM.2015.2428998},
  abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
  keywords = {Audio databases,event detection,Event detection,Licenses,machine intelligence,Microphones,Music,pattern recognition,Speech,Speech recognition},
  file = {files/146/Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf;files/147/7100934.html}
}

@article{tanDynamicTimeWarping2015,
  title = {Dynamic Time Warping and Sparse Representation Classification for Birdsong Phrase Classification Using Limited Training Data},
  author = {Tan, Lee N. and Alwan, Abeer and Kossan, George and Cody, Martin L. and Taylor, Charles E.},
  year = {2015},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {137},
  number = {3},
  pages = {1069--1080},
  issn = {0001-4966},
  doi = {10.1121/1.4906168},
  urldate = {2023-03-21},
  langid = {english},
  file = {files/84/Tan et al. - 2015 - Dynamic time warping and sparse representation cla.pdf}
}

@inproceedings{thakurRapidBirdActivity2017,
  title = {Rapid Bird Activity Detection Using Probabilistic Sequence Kernels},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Thakur, Anshul and Jyothi, R. and Rajan, Padmanabhan and Dileep, A.D.},
  year = {2017},
  month = aug,
  pages = {1754--1758},
  publisher = {{IEEE}},
  address = {{Kos, Greece}},
  doi = {10.23919/EUSIPCO.2017.8081510},
  urldate = {2023-03-21},
  abstract = {Bird activity detection is the task of determining if a bird sound is present in a given audio recording. This paper describes a bird activity detector which utilises a support vector machine (SVM) with a dynamic kernel. Dynamic kernels are used to process sets of feature vectors having different cardinalities. Probabilistic sequence kernel (PSK) is one such dynamic kernel. The PSK converts a set of feature vectors from a recording into a fixed-length vector. We propose to use a variant of PSK in this work. Before computing the fixed-length vector, cepstral mean and variance normalisation and short-time Gaussianization is performed on the feature vectors. This reduces environment mismatch between different recordings. Additionally, we also demonstrate a simple procedure to speed up the proposed method by reducing the size of fixed-length vector. A speedup of almost 70\% is observed, with a very small drop in accuracy. The proposed method is also compared with a random forest classifier and is shown to outperform it.},
  isbn = {978-0-9928626-7-1},
  langid = {english},
  keywords = {Audio recording,Birds,Kernel,Mel frequency cepstral coefficient,Phase shift keying,Probabilistic logic,Training},
  file = {files/106/Thakur et al. - 2017 - Rapid bird activity detection using probabilistic .pdf;files/122/8081510.html}
}

@article{tothConvolutionalNeuralNetworks2016,
  title = {Convolutional {{Neural Networks}} for {{Large-Scale Bird Song Classification}} in {{Noisy Environment}}},
  author = {T{\'o}th, B{\'a}lint P{\'a}l and Czeba, B{\'a}lint},
  year = {2016},
  abstract = {This paper describes a convolutional neural network based deep learning approach for bird song classification that was used in an audio record-based bird identification challenge, called BirdCLEF 2016. The training and test set contained about 24k and 8.5k recordings, belonging to 999 bird species. The recorded waveforms were very diverse in terms of length and content. We converted the waveforms into frequency domain and splitted into equal segments. The segments were fed into a convolutional neural network for feature learning, which was followed by fully connected layers for classification. In the official scores our solution reached a MAP score of over 40\% for main species, and MAP score of over 33\% for main species mixed with background species.},
  langid = {english},
  file = {files/72/Tóth et Czeba - Convolutional Neural Networks for Large-Scale Bird.pdf}
}

@article{zhaoAutomatedBirdAcoustic2017,
  title = {Automated Bird Acoustic Event Detection and Robust Species Classification},
  author = {Zhao, Zhao and Zhang, Sai-hua and Xu, Zhi-yong and Bellisario, Kristen and Dai, Nian-hua and Omrani, Hichem and Pijanowski, Bryan C.},
  year = {2017},
  month = may,
  journal = {Ecological Informatics},
  volume = {39},
  pages = {99--108},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2017.04.003},
  urldate = {2023-03-21},
  abstract = {Non-invasive bioacoustic monitoring is becoming increasingly popular for biodiversity conservation. Two automated methods for acoustic classification of bird species currently used are frame-based methods, a model that uses Hidden Markov Models (HMMs), and event-based methods, a model consisting of descriptive measurements or restricted to tonal or harmonic vocalizations. In this work, we propose a new method for automated field recording analysis with improved automated segmentation and robust bird species classification. We used a Gaussian Mixture Model (GMM)-based frame selection with an event-energy-based sifting procedure that selected representative acoustic events. We employed a Mel, band-pass filter bank on each event's spectrogram. The output in each subband was parameterized by an autoregressive (AR) model, which resulted in a feature consisting of all model coefficients. Finally, a support vector machine (SVM) algorithm was used for classification. The significance of the proposed method lies in the parameterized features depicting the speciesspecific spectral pattern. This experiment used a control audio dataset and real-world audio dataset comprised of field recordings of eleven bird species from the Xeno-canto Archive, consisting of 2762 bird acoustic events with 339 detected ``unknown'' events (corresponding to noise or unknown species vocalizations). Compared with other recent approaches, our proposed method provides comparable identification performance with respect to the eleven species of interest. Meanwhile, superior robustness in real-world scenarios is achieved, which is expressed as the considerable improvement from 0.632 to 0.928 for the F-score metric regarding the ``unknown'' events. The advantage makes the proposed method more suitable for automated field recording analysis.},
  langid = {english},
  file = {files/86/Zhao et al. - 2017 - Automated bird acoustic event detection and robust.pdf}
}

@article{attentionIsAllYouNeed,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}